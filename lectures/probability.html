<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-01-04 Thu 21:45 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Probability Basics for Robotics</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Matthew Elwin">
<link rel="stylesheet" href="./../pubme.css" type="text/css"/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="./../index.html"> HOME </a>
</div><div id="content">
<header>
<h1 class="title">Probability Basics for Robotics</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orge3b04d0">Why Probability?</a></li>
<li><a href="#org6f20c9b">Random Experiments</a>
<ul>
<li><a href="#org2c035f3">Examples</a></li>
<li><a href="#org0f528a6">Defining Random Experiments</a></li>
</ul>
</li>
<li><a href="#org9f3da94">Statistical Regularity</a>
<ul>
<li>
<ul>
<li><a href="#org3e5178a">Example: roll a die \(n\) times</a></li>
</ul>
</li>
<li><a href="#org037ecfd">Some Properties of statistical regularity</a></li>
<li><a href="#orge808e65">Discrete and Continuous Sample Spaces</a></li>
<li><a href="#orgadfd98a">Events:</a></li>
</ul>
</li>
<li><a href="#org9be097b">Probability Axioms</a></li>
<li><a href="#orgeca8ca2">Venn Diagrams</a>
<ul>
<li><a href="#org5e930a7">Important Corollaries:</a></li>
</ul>
</li>
<li><a href="#org1406549">Conditional Probability</a>
<ul>
<li><a href="#orgd22a54b">Example:</a></li>
</ul>
</li>
<li><a href="#org81e009a">Theorem on total probability</a></li>
<li><a href="#org4fe72ab">Bayes' Theorem</a>
<ul>
<li><a href="#org5c9dae6">Terminology:</a></li>
<li><a href="#org000630d">Graphical Approach:</a></li>
</ul>
</li>
<li><a href="#orgebc635f">Independence</a></li>
<li><a href="#org6491524">Random Variable</a>
<ul>
<li><a href="#orge45f3e3">Example:</a></li>
<li><a href="#orgbfbc845">Discrete Random Variables</a></li>
</ul>
</li>
<li><a href="#org83f77ed">Example</a>
<ul>
<li><a href="#orgd3d5674">Continuous Random Variables</a></li>
</ul>
</li>
<li><a href="#orge249ab7">Expected Value</a>
<ul>
<li><a href="#orgd7814b8">Discrete</a>
<ul>
<li><a href="#org684fbb5">Applications</a></li>
</ul>
</li>
<li><a href="#org4897106">Continuous</a></li>
<li><a href="#org8289da8">Two important Continuous Random Variables:</a>
<ul>
<li><a href="#orgd963d74">Uniform Random variable: any value is equally likely</a></li>
<li><a href="#org06efadc">Gaussian Random Variable</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge674c08">Functions of Random Variables:</a></li>
<li><a href="#orge2d6a4e">Joint Distributions</a>
<ul>
<li><a href="#org75a1aa7">Joint Moments/expected value</a></li>
</ul>
</li>
<li><a href="#orgf733648">More conditional probability</a></li>
<li><a href="#orgea642fa">More linear transformations:</a></li>
<li><a href="#orgd231de5">Conditional Independence</a></li>
<li><a href="#org8e7551f">Resources</a></li>
</ul>
</div>
</nav>
<div id="outline-container-orge3b04d0" class="outline-2">
<h2 id="orge3b04d0">Why Probability?</h2>
<div class="outline-text-2" id="text-orge3b04d0">
<ul class="org-ul">
<li>Until now, we have discussed deterministic models
<ul class="org-ul">
<li>These models are wrong (just wait until you see how poorly our forward kinematics predicts the robot movement)
<ul class="org-ul">
<li>We could improve the models, such as by adding mass/inertia/dynamics</li>
</ul></li>
<li><p>
Ultimately, we will reach the limit of our ability to model everything precisely
</p>
<ul class="org-ul">
<li>Maybe a perfect model is possible if only we knew more, but probably not.
<ul class="org-ul">
<li>(Is the system actually stochastic, or does it just appear to be stochastic due to our lack of understanding of its dynamics?)</li>
</ul></li>
<li>We are definitely uncertain about some quantities (listed in roughly decreasing order of importance)
<ul class="org-ul">
<li>Friction</li>
<li>Manufacturing tolerances</li>
<li>Air Resistance</li>
<li>Radio waves</li>
<li>The earth is spinning and rotating around the sun</li>
<li>Gravity is different in different places</li>
</ul></li>
<li>Sometimes we can engineer products to reduce uncertainty (such as not relying on the gain of a transistor in a circuit)</li>
<li>Sometimes we engineer products with sensors so that the product works despite not having perfect models (this is feedback control)</li>
</ul>
<ul class="org-ul">
<li>Probability theory enables us and the robots we program to reason about uncertain and random situations.
<ul class="org-ul">
<li>Applications in Sensor Fusion, Localization, Mapping, and Machine Learning</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org6f20c9b" class="outline-2">
<h2 id="org6f20c9b">Random Experiments</h2>
<div class="outline-text-2" id="text-org6f20c9b">
<ul class="org-ul">
<li>A procedure that has an unpredictable outcome despite being repeated under the same conditions.
<ul class="org-ul">
<li>Obtaining conditions for an exactly repeatable experiment is quite difficult or impossible.</li>
<li>How do we know the conditions are the same?
<ul class="org-ul">
<li>Conditions are the same up until the best of our ability to measure and control the environment</li>
</ul></li>
</ul></li>

<li>Sample Space: Set containing all of the potential outcomes of a random experiment
<ul class="org-ul">
<li>The sample space is where the underlying randomness happens</li>
<li>Randomness is modeled as "run the experiment, which sample space element did we get?"</li>
<li>Items in the sample space can be anything</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org2c035f3" class="outline-3">
<h3 id="org2c035f3">Examples</h3>
<div class="outline-text-3" id="text-org2c035f3">
<ul class="org-ul">
<li>Rolling a single die
<ul class="org-ul">
<li>Possible outcomes, a number 1 - 6: these are the outcomes and they
form the sample space</li>
<li>The sample space could also consist of (literally) the face of the die that is showing.
<ul class="org-ul">
<li>\(S=\{⚀,⚁,⚂,⚃,⚄,⚅\}\)</li>
</ul></li>
<li>For this example the sample space is finite</li>
</ul></li>

<li>Breaking a stick in half at a random location
<ul class="org-ul">
<li>Could define the sample space as a set of pairs of stick halves</li>
<li>Sample space could be set of all possible distances from the left end where the break occurred</li>
<li>For this example, the sample space is uncountably infinite (i.e., a real number)
<ul class="org-ul">
<li>(Aside: Is it really though? The stick is made up of a finite number of atoms. Although maybe the space between those atoms can vary infinitely&#x2026;)</li>
</ul></li>
</ul></li>
<li>At the end of the day, no matter how complicated the system is, there is always
<ol class="org-ol">
<li>A random experiment</li>
<li>A sample space</li>
</ol></li>
</ul>
</div>
</div>

<div id="outline-container-org0f528a6" class="outline-3">
<h3 id="org0f528a6">Defining Random Experiments</h3>
<div class="outline-text-3" id="text-org0f528a6">
<ul class="org-ul">
<li>Specify a random experiment by defining
<ol class="org-ol">
<li>A procedure of what happens:
<ul class="org-ul">
<li>Roll a die one time</li>
<li>Command diff-drive robot to drive forward with speed of 0.5 m/s for 2 seconds</li>
</ul></li>
<li>At least one observation, for example
<ul class="org-ul">
<li>Note the position of the robot relative to the world coordinate frame)</li>
<li>Observe if the number showing at the top of the die is a 1</li>
<li>Observe if the number showing at the top of the die is even or odd</li>
<li>Observe the distance the robot has traveled from its initial to final position</li>
<li>Observe the (theta, x, y) pose of the robot in a world coordinate frame</li>
</ul></li>
</ol></li>
<li>Define the sample space \(S\) as the set of possible outcomes (i.e., possible things you have observed)</li>
<li>Thus all outcomes in the sample space are mutually exclusive (no two outcomes occur at the same time)
<ol class="org-ol">
<li>For a single die and observing the number at the top it is \(S=\{1,2,3,4,5,6\}\)</li>
<li>For a single die and observing if the number at the top of the die is a 1 it is \(S=\{1 \text{is showing}, 1 \text{is not showing}\}\)</li>
<li>For the robot driving forward when we measure distance it is the set of positive real numbers (since distance is positive)
<ul class="org-ul">
<li>You can also define the sample space as all real numbers, with some outcomes being impossible</li>
</ul></li>
<li>For the robot driving forward and measuring the pose it is multimensional cartesian product of intervals: \((-\pi, \pi) \times \mathbb{R} \times  \mathbb{R}\).</li>
</ol></li>
<li>Sample space depends not just on the experiment but also on the observation:
<ul class="org-ul">
<li>Toss a coin twice, and note the sequence: \(S = {HH, HT, TH, TT}\)</li>
<li>Toss a coin twice, and count the number of heads: \(S ={0, 1, 2}\)</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org9f3da94" class="outline-2">
<h2 id="org9f3da94">Statistical Regularity</h2>
<div class="outline-text-2" id="text-org9f3da94">
<ul class="org-ul">
<li>Why do we care about probability at all?</li>
<li>After many repetitions, average outcome of a random experiment should be the same</li>
<li>Relative frequency: the number of times an outcome occurs divided by number of experiments
that were run</li>
<li>Statistical regularity is important for connecting theoretical notions of probability to real-world outcomes</li>
</ul>
</div>
<div id="outline-container-org3e5178a" class="outline-4">
<h4 id="org3e5178a">Example: roll a die \(n\) times</h4>
<div class="outline-text-4" id="text-org3e5178a">
<ul class="org-ul">
<li>Relative frequency: number of times a 1 appears divided by \(n\) times</li>
<li>As \(n\) goes to infinity, relative frequency of the die goes to \(1/6\),
just as 1 out of 6 elements in the sample space is a 1</li>
<li>This is the probability of rolling a 1.</li>
</ul>
<ul class="org-ul">
<li>If you can use a computer to repeat an experiment many times, you can sometimes brute-force
an estimate for probability.</li>
<li>In the real world, it is (technically) impossible to repeat the same exact experiment more than once,
but we rely on controlling as many of the initial conditions as possible.</li>
</ul>
</div>
</div>

<div id="outline-container-org037ecfd" class="outline-3">
<h3 id="org037ecfd">Some Properties of statistical regularity</h3>
<div class="outline-text-3" id="text-org037ecfd">
<ol class="org-ol">
<li>Each outcome has relative frequency between 0 and 1
<ul class="org-ul">
<li>You can't get an outcome more than the number of times you run the experiment</li>
</ul></li>
<li>The sum of the number of number of occurrences across possible outcomes is \(n\) (and thus the sum of relative frequencies is 1)
<ul class="org-ul">
<li>Every experiment yields an outcome</li>
</ul></li>
<li>The frequency of an event is the sum of the individual frequencies of individual outcomes
<ul class="org-ul">
<li>relative frequency of odd dice rolls = relative frequency of rolling a 1 + rolling a 3 + rolling a 5</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-orge808e65" class="outline-3">
<h3 id="orge808e65">Discrete and Continuous Sample Spaces</h3>
<div class="outline-text-3" id="text-orge808e65">
<ul class="org-ul">
<li>Discrete: Sample space is a finite or countably infinite
<ul class="org-ul">
<li>Countably infinite sample space example:
Flip a coin until it is heads.  Observe the numbers of tails</li>
<li>{0, 1, 2, 3, &#x2026;}</li>
</ul></li>
<li>Continuous: Sample space is uncountable 
<ul class="org-ul">
<li>Spin a wheel, observe the angle</li>
<li>Throw a dart on a board, note its x and y coordinates</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgadfd98a" class="outline-3">
<h3 id="orgadfd98a">Events:</h3>
<div class="outline-text-3" id="text-orgadfd98a">
<ul class="org-ul">
<li>Informally: anything that can happen as a result of an experiment that we might want to know the probability of.</li>
<li>An event is a subset of the sample space.
<ul class="org-ul">
<li>The event occurs if any outcome in the event set occurs</li>
</ul></li>
<li>The certain event: the full sample space, it occurs if any of the outcomes</li>
<li>The impossible event - never occurs, the empty set.</li>
<li>Event Class:
<ul class="org-ul">
<li>Contains the events of interest, which are assigned probabilities</li>
<li>Compliments, countable unions and countable intersections of events are also in the event class</li>
</ul></li>
<li>For discrete sample spaces: this is (usually) the set of all subsets of \(S\) (this does not hold for continuous sample spaces)</li>
<li>For continuous sample spaces: complements, countable unions and countable intersections of intervals of the real line \((a, b] \text{ or } (-\infty, b]\)
<ul class="org-ul">
<li>Not set of all subsets (for technical reasons).</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org9be097b" class="outline-2">
<h2 id="org9be097b">Probability Axioms</h2>
<div class="outline-text-2" id="text-org9be097b">
<ul class="org-ul">
<li>The Assumptions:
<ul class="org-ul">
<li>We have defined a random experiment, whose possible outcomes are in the set \(S\)</li>
<li>An event class \(F\) consisting of subsets of \(S\) have been defined.</li>
<li>Each event \(A\) in \(F\) has been assigned a number \(P[A]\)</li>
</ul></li>
<li>The axioms:
<ol class="org-ol">
<li>\(0 \leq P[A]\)</li>
<li>\(P[S] = 1\) (that is, probability of something in the sample space occuring is 1).</li>
<li>Probability of an infinite union of mutually exclusive events is the sum of the individual probabilities:
<ul class="org-ul">
<li>If \(A \cap B = \emptyset\), then \(P[ A \cup B] = P[A] + P[B]\) (good enough for finite sample spaces)</li>
<li>If \(A_1, A_2, \dots\) is a countable sequence such that \(A_i \cap A_j = \emptyset\) for all \(i \neq j\) then
\(P[\bigcup_{k=1}^{\infty}A_k] = \sum_{k=1}^{\infty}P[A_k]\) (more general)</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgeca8ca2" class="outline-2">
<h2 id="orgeca8ca2">Venn Diagrams</h2>
<div class="outline-text-2" id="text-orgeca8ca2">
<ul class="org-ul">
<li>We can use Venn diagrams to visualize probability</li>
</ul>
</div>
<div id="outline-container-org5e930a7" class="outline-3">
<h3 id="org5e930a7">Important Corollaries:</h3>
<div class="outline-text-3" id="text-org5e930a7">
<ol class="org-ol">
<li>\(P[A^c] = 1 - P[A]\) (because \(S = A \cup A^c\), \(P[S] = 1\), and \(P[A \cup A^c] = P[A] + P[A^c]\))</li>
<li>\(P[A] <= 1\): (from 1, and the fact that \(P[A^c] \geq 0\)).</li>
<li>\(P[\emptyset] = 0\) (Let \(A = S\) then \(A^c = \emptyset\), result follows from 1.)</li>
<li>\(P[A \cup B] = P[A] + P[B] - P[A \cap B]\)</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org1406549" class="outline-2">
<h2 id="org1406549">Conditional Probability</h2>
<div class="outline-text-2" id="text-org1406549">
<p>
How does knowledge about event \(B\) occurring change the probability of event \(A\)?
</p>
<ul class="org-ul">
<li>\(P[A | B] = \frac{P[A \cap B]}{P[B]}, P[B] > 0\).</li>
<li>If \(B\) has occurred then \(A\) can only occur if \(A\) and \(B\) has occurred</li>
<li>Thus the sample space has been reduced to a subset of \(B\).</li>
<li>For fixed \(B\), \(P[A | B]\) is itself a  probability over this reduced sample space</li>
</ul>
</div>
<div id="outline-container-orgd22a54b" class="outline-3">
<h3 id="orgd22a54b">Example:</h3>
<div class="outline-text-3" id="text-orgd22a54b">
<p>
Random Experiment: Flip two coins, count the number of Heads
</p>
<ul class="org-ul">
<li>Sample space \(\{0, 1, 2\}\)</li>
<li>Event Space is \(\{\emptyset, \{0\}, \{1\}, \{2\}, \{0, 1\}, \{0, 2\}, \{1, 2\}, \{0, 1, 2\}\}\)</li>
<li>Probability:
\(P[0] = \frac{1}{4}\), \(P[1] = \frac{1}{2}\), \(P[2]= \frac{1}{4}\)</li>
<li><p>
Question
What is the probability of there having been 1 head given that either 1 or two heads were obtained:
</p>
\begin{align}
P[\{1\} | \{1, 2\}] &= \frac{P[\{1\} \cap \{1, 2\}]}{P[\{1, 2\}]} \notag \\
                    &= \frac{P[\{1\}]}{P[\{1\}\cup\{2\}]} \notag \\
                    &= \frac{P[{1}]}{P[{1}] + P[{2}]} \notag \\
                    &= \frac{1}{2}/\frac{3}{4}\notag \\
                    &= \frac{2}{3} \notag
\end{align}</li>
<li>Note: the probability is higher then the probability of \(\{1\}\) on its own, which makes sense because we've eliminated the possibility of 0 heads.</li>

<li>We can also write \(P[A \cap B] = P[A | B]P[B] = P[B | A] P[A]\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org81e009a" class="outline-2">
<h2 id="org81e009a">Theorem on total probability</h2>
<div class="outline-text-2" id="text-org81e009a">
<ul class="org-ul">
<li>A practical way of computing probabilities when only conditionals are known.</li>
<li>\(B_1, \dots B_n\) are mutally exclusive events whose union is the entire sample space
\(P[A] = P[A | B_1]P[B_1] + P[A | B_2]P[B_2] + \dots + P[A | B_n]P[B_n]\)</li>
</ul>
</div>
</div>


<div id="outline-container-org4fe72ab" class="outline-2">
<h2 id="org4fe72ab">Bayes' Theorem</h2>
<div class="outline-text-2" id="text-org4fe72ab">
<ul class="org-ul">
<li>The problem: what is the probability that a hypthesis \(H\) is correct given evidence \(E\)?</li>
<li>So, there is a random experiment, we observe event \(E\), how likely is event \(H\)?</li>
<li>From conditional probability laws: \(P[H | E] = \frac{P[H \cap E]}{P[E]}\).  However,
<ul class="org-ul">
<li>What if we don't know \(P[H \cap E]\)? Or \(P[E]\)?</li>
<li>For example (all numbers made up):
What is the probability that there is a fire in the Tech Building, given
that the fire alarm on?
<ul class="org-ul">
<li>This question is asking \(P[\text{Fire in tech} | \text{alarm}]\) (so \(H = \text{fire in tech}\), \(E = \text{alarm}\)).</li>
<li>But, the probability of there being a fire in tech and the alarm going off is unknown.
-Why not?  What about all the times the alarm goes off and there is no fire?</li>
<li>Assume that fire alarms are very accurate.  Given that there is a fire,
the probability of a fire alarm going off is 0.90. Thus \(P[E | H] = 0.90\)</li>
<li>I also know that the fire alarm does not go on that often there is no fire.
Thus \(P[E | H^c] = 0.2\)</li>
<li>Fires are rare \(P[H] = 0.001\)</li>
</ul></li>
</ul></li>
<li><p>
From conditional probability formulas: \(P[H | E]P[E] = P[E | H] P[H]\),
</p>
<ul class="org-ul">
<li>This is Bayes' theorem:</li>
</ul>
\begin{equation}
P[H | E] = \frac{P[E | H] P[H]}{P[E]}.
\end{equation}
<ul class="org-ul">
<li>Often we use law of total probability to compute \(P[E]\)
<ul class="org-ul">
<li>\(P[E] = P[E | H]P[H] + P[E | H^c]P[H^c]\)</li>
</ul></li>
</ul></li>
<li>Therefore, using the numbers I just stated:
probability of fire, given fire alarm is \(0.9*0.001/(0.9*0.001 + 0.2*0.999) = 0.004\)</li>
<li>The reason this is so low, even though fire alarms are accurate and don't go off by accident too often,
is because, to begin with, there is a very small chance of a fire.</li>
</ul>
</div>
<div id="outline-container-org5c9dae6" class="outline-3">
<h3 id="org5c9dae6">Terminology:</h3>
<div class="outline-text-3" id="text-org5c9dae6">
<ul class="org-ul">
<li>Hypothesis: \(H\) (There is a fire in Tech)</li>
<li>Evidence: \(E\) (The fire alarm is going off)</li>
<li>\(P[H]\) is the <i>prior</i> This is the probability of the hypothesis is true
in the absence of any evidence.  (Probability of a fire in Tech)
<ul class="org-ul">
<li>We are assumed to have some prior understanding of the world</li>
</ul></li>
<li>\(P[E | H]\) is the likelihood. How likely would the evidence be were our hypothesis
correct. (Probability of fire alarm going off given that there is a fire).</li>
<li>\(P[H | E]\) is the posterior - How likely is our hypothesis, now that we've seen the
evidence (this is what we are computing</li>
<li>\(P[E]\) - A "normalization factor".  By seeing the evidence we are restricting the size
of the sample space to only those samples consistent with the Evidence event having occurred.</li>
</ul>
</div>
</div>

<div id="outline-container-org000630d" class="outline-3">
<h3 id="org000630d">Graphical Approach:</h3>
<div class="outline-text-3" id="text-org000630d">
<ul class="org-ul">
<li><p>
Linus is a computer programmer. He likes writing bash scripts and is a git guru.
What is the probability that Linus uses Linux?
</p>
<ul class="org-ul">
<li>H - Linus uses Linux</li>
<li>Not H - Linus uses Windows (or something else)</li>
<li>E - Linus is a computer programmer who likes writing bash scripts and is a git guru.</li>
</ul>

<figure>
<object type="image/svg+xml" data="../images/bayes.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>

</figure>

<ul class="org-ul">
<li>Bayes' Theorem tells us that, even though that there is a high probability
that Linux users fit the evidence (they program, like writing bash scripts and are
masters at using git), that is P(E | H) is large,  and Windows/other users likely don't
fit the evidence (that is P(E | not H) is small), the prior probability that
a user is a Linux user P(H) is so low that there is actually a greater
chance of Linus using Windows/Other rather than Linux.</li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orgebc635f" class="outline-2">
<h2 id="orgebc635f">Independence</h2>
<div class="outline-text-2" id="text-orgebc635f">
<p>
Knowledge that event B occurs does not affect the chance of A occuring:
\(P[A \cap B] = P[A] P[B]\), which implies that \(P[A|B] = P[A]\) and \(P[B|A] = P[B]\) (because of the law on conditional probability)
</p>

<p>
Example:
</p>
<ol class="org-ol">
<li>Random Experiment: We flip a quarter and a nickel and observe whether they are
heads or tails.  
<ul class="org-ul">
<li>The sample space is \(S = \{(Q_H, N_H), (Q_H, N_T), (Q_T, N_H), (Q_T, N_T)\}\)</li>
<li>Event \(A = \{(Q_H, N_H), (Q_H, N_T)\) "The quarter is heads"</li>
<li>Event \(B = \{(Q_H, N_T), (Q_T, N_T)\) "The nickle is tails"</li>
<li>Event \(C = \{(Q_H, N_H)\}\) "All heads"</li>
<li>\(P[A] = 1/2\), \(P[B] = 1/2\), \(P[C]=1/4\)</li>
<li>\(P[A \cap B] = P[\{Q_H, N_T\}] = 1/4 = P[A] P[B]\), therefore \(A\) and \(B\) are 
independent.</li>
<li>Also, note that \(P[A | B] = P[A] =  1/2\) (if event B has occurred,
then 1 out of the two outcomes in event \(B\) would lead to event \(A\) also occuring.</li>
<li>\(P[A \cap C] = P[\{(Q_H, N_H)\}] = 1/4 \neq P[A]P[C]\) so \(A\) and \(C\) are not independent</li>
</ul></li>

<li>Mutually exclusive events with non-zero probability can never be independent.
<ul class="org-ul">
<li>Mutually exclusive means that "if A occurs than B cannot occur and vice versa"</li>
<li>Thus, knowing about the occurence or non-occurance of A gives you information about B</li>
<li>P[A and B] = 0 for mutally exclusive events, which means P[A] or P[B] = 0.</li>
</ul></li>
<li>For multiple events, independence requires:
For \(k \in \{2, \dots, n\}\):
\(P[A_{i1} \cap ... A_{ik}] = P[A_{i1}]...P[A_{ik}]\)</li>
</ol>
</div>
</div>

<div id="outline-container-org6491524" class="outline-2">
<h2 id="org6491524">Random Variable</h2>
<div class="outline-text-2" id="text-org6491524">
<ul class="org-ul">
<li>Informally: A function that assigns a real number to each outcome in the sample space.</li>
<li>Not a Variable
<ul class="org-ul">
<li>Actually a function from the sample space to real numbers</li>
</ul></li>
<li>Not Random
<ul class="org-ul">
<li>Actually a deterministic mapping. The randomness is from which outcome
in the sample space occurs after performing the experiment</li>
</ul></li>
<li>Why Random Variables?
<ul class="org-ul">
<li>Allow us to look at different aspects of the same random experiment</li>
<li>Provide an easier way to assign probabilities to elements of our sample space</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orge45f3e3" class="outline-3">
<h3 id="orge45f3e3">Example:</h3>
<div class="outline-text-3" id="text-orge45f3e3">
<ul class="org-ul">
<li><p>
Flip two coins. \(\xi \in S\). 
\(X(\xi)\) is a random variable counting the number of heads. 
\(Y(X(\xi))\) is a random variable that is twice the number of heads
</p>
<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(\xi\)</th>
<th scope="col" class="org-right">HH</th>
<th scope="col" class="org-right">HT</th>
<th scope="col" class="org-right">TH</th>
<th scope="col" class="org-right">TT</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">X(&xi;)</td>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Y(&xi;)</td>
<td class="org-right">4</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table></li>

<li>Function of a random variable is also a random variable</li>
<li>If the sample space is already numerical, the random variable is just \(X(\xi) = \xi\)</li>
<li>So \(X : S \to S_x\), \(S_x \subset \mathbb{R}\), thus \(S\) is the domain and \(S_X\) is the range of the random variable.</li>
</ul>
</div>
</div>

<div id="outline-container-orgbfbc845" class="outline-3">
<h3 id="orgbfbc845">Discrete Random Variables</h3>
<div class="outline-text-3" id="text-orgbfbc845">
<ul class="org-ul">
<li>The random variable maps items in the sample space to a finite set of values</li>
<li>Probability for events \(A_k = \{\xi: X(\xi) = x_k\}\)</li>
<li>Probability mass function (pmf) 
\(p_x(x) = P[X = x] = P[\{\xi : X(\xi) = x\}]\), where \(x\) is a real number.</li>
<li>The event \(A_k = \{\xi : X(\xi) = x_k\}\) is the equivalent event to \(X(\xi) = x_k\)</li>
</ul>


<ul class="org-ul">
<li>Properties of pmf (The events \(A_k\)  partition \(S\), which each partition mapping to a value of the random variable).
<ol class="org-ol">
<li>\(p_x(x) >= 0\)</li>
<li>\(\sum_{x \in S_x} p_X(x) = \sum_{k}p_X(x_k) = \sum_{k}P[A_k] = 1\) (This is because the \(A_k\) partition \(S\))</li>
<li>\(P[X \in B] = \sum_{x\in B}p_X(x),\text{ where }B \subset S_x\) - This is because the events in \(B\) are a union of elementary events.</li>
</ol></li>

<li>pmf provides probabilities for all events from the range of the random variable.</li>

<li>Conditional pmf
\(p_x(x|C) = P[X = x | C] = \frac{P[{X=x}\cap C]}{P[C]}\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org83f77ed" class="outline-2">
<h2 id="org83f77ed">Example</h2>
<div class="outline-text-2" id="text-org83f77ed">

<figure>
<object type="image/svg+xml" data="../images/binomial_pmf.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>

<figcaption><span class="figure-number">Figure 2: </span>A probability mass function for a binomial distribution.</figcaption>
</figure>
</div>
<div id="outline-container-orgd3d5674" class="outline-3">
<h3 id="orgd3d5674">Continuous Random Variables</h3>
<div class="outline-text-3" id="text-orgd3d5674">
<ul class="org-ul">
<li>\(X(\xi)\) is a function from \(S\) to \(\mathbb{R}\) such that \(A_b = \{\xi : X(\xi) \leq b\}\) is in the event class for every \(b\) in \(\mathbb{R}\).
<ul class="org-ul">
<li>Basically, there is an event consisting of all the sample space outcomes that \(X\) maps to a value \(\leq\) b, for any b</li>
</ul></li>

<li><p>
Cumulative Distribution Function (cdf) for random variable X is the probability of the event that \(x \leq X\).
</p>
\begin{equation}
F_X(x) = P[X \leq x], -\infty < x < \infty.
\end{equation}</li>
<li>\(P[a < X \leq b] = F_X(b) - F_X(a)\)</li>
<li>For continuous random variables \(P[X = x] = 0\).</li>
</ul>


<ul class="org-ul">
<li>All types of random variables (continuous, discrete, and mixed) can be described in terms of a cdf.</li>
<li>We'll use the pmf for discrete random variables and not worry about its cdf (its a staircase function)</li>
<li>For continuous random variables, we are more used to dealing with a probability density function:
\(F_x(x) = \int_{-\infty}^x f(t)dt\), although this does not always exist so is less general.</li>
</ul>

<p>
Probability Density function
</p>
<ul class="org-ul">
<li>When it exists \(f_x(x) = \frac{dF_X(x)}{dx}\)</li>
<li>\(f_x(x) >= 0\)</li>
<li>\(P[a \leq X \leq b] = \int_{a}^{b}f_X(x)dx\)</li>
<li>pdf completely specifies behavior of continuous random variables</li>

<li>Conditional cdf</li>
</ul>
<p>
\(F_x(x | C) = \frac{P[{X \leq x} \cap C]}{P[C]}, \text{ if } P[C]>0\)
</p>
<ul class="org-ul">
<li><p>
Conditional pdf is derivative of conditional cdf with respect to x:
</p>
\begin{equation}
f_X(x|C) = \frac{d F_x(x | C)}{dx}
\end{equation}</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge249ab7" class="outline-2">
<h2 id="orge249ab7">Expected Value</h2>
<div class="outline-text-2" id="text-orge249ab7">
</div>
<div id="outline-container-orgd7814b8" class="outline-3">
<h3 id="orgd7814b8">Discrete</h3>
<div class="outline-text-3" id="text-orgd7814b8">
<p>
Let \(S_x\) be the range of the random variable \(X\). Then
</p>
\begin{equation}
m_x = E[X(\xi)] = \sum_{x \in S_x}xp_x(x) = \sum_{k}x_kp_x(x_k)
\end{equation}

<ul class="org-ul">
<li>The "average of X" after observing X after many experiments.
<ul class="org-ul">
<li>Based on the relative frequency interpretation</li>
<li>\(p_x(x) = N(x)/n, n \to \infty\), then \(m_x = \sum_k x N(x_k)/n = \frac{1}{n}\sum_k x_k N(x_k)\)</li>
</ul></li>
<li>Thus, run the experiment many times, take the mean over each run, it should converge to E[X]</li>
<li>It does NOT mean the value you could expect from any given experiment:
For example, flip a coin, \(X(H) = 0\), \(X(T) = 1\), assuming a fair coin \(E[X] = 1/2\).</li>
</ul>
</div>

<div id="outline-container-org684fbb5" class="outline-4">
<h4 id="org684fbb5">Applications</h4>
<div class="outline-text-4" id="text-org684fbb5">
<ol class="org-ol">
<li>How much money should I bet in  a game?
<ul class="org-ul">
<li>Experiment: flip coin 2 times.  Random variable is the payout.</li>
<li>\(X(HH) = 2\), \(X(TT) = -1\), \(X(TH) = 0\), \(X(HT) = 0\)</li>
<li>\(p_X(2) = 1/4\), \(p_X(-1) = 1/4\), \(p_X(0) = 1/2\).</li>
<li>\(E[X] = (2)1/4 - (1)1/4 + (0)1/2) = 3/4\)
<ul class="org-ul">
<li>This means if you play the game many times you can expect end up with 3/4 times the number of times you played</li>
</ul></li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org4897106" class="outline-3">
<h3 id="org4897106">Continuous</h3>
<div class="outline-text-3" id="text-org4897106">
<p>
Expected value of \(Y = g(X)\)
\(E[Y]= \int_{-\infty}^{\infty}g(x)f_x(x)dx\)
</p>
<ul class="org-ul">
<li>This is expected value of X when \(Y = X\) and corresponds to the "center of mass" of the pdf</li>
<li>Variance: \(g(X) = (X - E[X])^2\) - corresponds to the spread about the "center of mass" of the pdf</li>
</ul>
</div>
</div>
<div id="outline-container-org8289da8" class="outline-3">
<h3 id="org8289da8">Two important Continuous Random Variables:</h3>
<div class="outline-text-3" id="text-org8289da8">
</div>
<div id="outline-container-orgd963d74" class="outline-4">
<h4 id="orgd963d74">Uniform Random variable: any value is equally likely</h4>
<div class="outline-text-4" id="text-orgd963d74">
<p>
\(f_U(x)= \begin{cases}\frac{1}{b-a} & a \leq x \leq b \\ 0\end{cases}\)
</p>
</div>
</div>

<div id="outline-container-org06efadc" class="outline-4">
<h4 id="org06efadc">Gaussian Random Variable</h4>
<div class="outline-text-4" id="text-org06efadc">
<ol class="org-ol">
<li>Lots of special properties: it often can model the sum of large numbers of independent random variables (Central Limit Theorem)</li>
<li>Completely determined by two parameters: mean and covariance</li>
</ol>
\begin{equation}
f_x(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{\frac{-(x-m)^2}{2\sigma^2}}
\end{equation}
<ol class="org-ol">
<li>What is the cdf of a gaussian? It involves the error function.</li>
</ol>
</div>
</div>
</div>
</div>


<div id="outline-container-orge674c08" class="outline-2">
<h2 id="orge674c08">Functions of Random Variables:</h2>
<div class="outline-text-2" id="text-orge674c08">
<p>
If \(Y = g(X)\) then, given a pdf or cdf of X, what is the equivalent pdf or cdf of g?
Let \(C\) be an event.
</p>
\begin{equation}
P[Y \in C] = P[g(X) \in C] = P[X \in \{e : g(X) \in C\}]
\end{equation}

<p>
For example:
\(Y = a X + b\)
Then
</p>
\begin{align}
P[Y \leq y] &= P[a X + b \leq y]\\ 
&= \begin{cases}P[X \leq \frac{y - b}{a}] & a > 0 \\ P[X \geq \frac{y-b}{a}] & a < 0\end{cases}.
\end{align}
<p>
From a CDF perspective, we write
</p>
\begin{equation}
F_Y(y) = \begin{cases} F_X(\frac{y-b}{a}) & a > 0 \\ 1 - F_X(\frac{y-b}{a}) & a < 0\end{cases}.
\end{equation}

<p>
To get equivalent PDF:
\(\frac{d F}{dy} = \frac{dF}{du}\frac{du}{dy}\), where \(u\) is the argument to \(F_X\)
</p>

<p>
After doing the math for both cases, the result is
\(f_Y(y) = \frac{1}{|a|}f_X(\frac{y-b}{a})\)
</p>

<ul class="org-ul">
<li>Exercise: prove that a linear function of a Gaussian is also a Gaussian.
<ul class="org-ul">
<li>Substitute PDF for a gaussian into the equation above, rearrange to show that it is also in the form of a Gaussian</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge2d6a4e" class="outline-2">
<h2 id="orge2d6a4e">Joint Distributions</h2>
<div class="outline-text-2" id="text-orge2d6a4e">
<ul class="org-ul">
<li>What happens when we have more than one random variable at a time?</li>
<li>These vector random variables map the sample space to a vector in \(R^n\)</li>
<li>For two random variables, the events can be viewed as regions in the plane</li>
<li>Probability is assigned via equivalent events:
<ul class="org-ul">
<li>B is a region in \(R^n\)</li>
<li>A is an event in the event class such that  \(A = X^{-1}(B) = \{\xi : (X_1(\xi), X_2(\xi), ...) \in B\}\)</li>
</ul></li>
<li>Thus \(P[X \in B] = P[A]\).</li>
<li>The concepts of pmf, cdf, and pdf are all extended from a single dimension</li>
<li>pmf: \(p_{X_1, ..., X_N}(x_1,\dots x_n) = P[\{X_1 = x_1\} \cap \{X_2 = x_2\} \cap \dots \{ X_n = x_n\}]\)
<ul class="org-ul">
<li>Marginal pmf - Find the probability of a single variable, regardless of the other variables:</li>
<li><p>
This is found by summing over all possibilities for the other values
</p>
\begin{align}
p_{X_j}(x_j) &= P[X = x_j \text { and } X_1, \dots X_{j-1}, X_{j+1}, \dots, X_n
           &= \sum_{x_1} \dots \sum_{x_{j-1}}\sum_{x_{j+1}}\dots\sum_{x_n}p_{X_1,\dots X_n}(x_1, \dots x_n)
\end{align}</li>
</ul></li>
<li>cdf: \(F_{XY}(X \leq x \text{ and } Y \leq y)\)</li>
<li>pdf: \(f_{x,y}(x,y) = \frac{\partial F_{XY}(x,y)}{\partial x \partial y}\).
<ul class="org-ul">
<li>Also, \(F_{X,Y}(x,y) = \int_{-\infty}^x\int_{-\infty}^yf_{XY}(u,v)dudv\)</li>
</ul></li>
<li>Marginal pdf: \(f_y(y) = \int_{-\infty}^{\infty}f_{XY}(u,y)du\)</li>

<li>The joint pdf OR cdf OR pmf is sufficient for finding all other probabilities (marginal and conditional).</li>
<li>Generally, the joint pdf cannot be found from the marginal pdfs alone</li>

<li>Two random variables are independent if and only if \(p_{xy}(x_j, y_k) = p_{x}(x_j)p_y(y_j)\).</li>
</ul>
</div>

<div id="outline-container-org75a1aa7" class="outline-3">
<h3 id="org75a1aa7">Joint Moments/expected value</h3>
<div class="outline-text-3" id="text-org75a1aa7">
<p>
Let Z = g(X_1, &#x2026; X_n).
</p>
<ul class="org-ul">
<li>for continuous Then \(E[Z] = \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}g(X_1, ... X_n)f_{X_1...X_n}(x_1, ..., x_n) dx_1 \dots dx_n\)</li>
<li>for discrete, replace integral with sum.</li>
<li>Joint moment of X and Y: \(E[X^j Y^k] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}x^j y^k f_{XY}(x,y) dx dy\)</li>
<li>When \(j = 0\) or \(k = 0\) we get the mean</li>
<li>When \(j = k = 1\) we get the correlation.  If two variables are not correlated, they are "orthogonal"
<ul class="org-ul">
<li>Subtracting out the mean yields the covariance \(E[(X - E[X])(Y-E[Y])]\).</li>
</ul></li>
<li>Covariance measures deviation from the mean.</li>
<li>Variables are uncorrelated if covariance is zero</li>
<li>Independence implies uncorrelated but not the opposite (unless jointly gaussian).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgf733648" class="outline-2">
<h2 id="orgf733648">More conditional probability</h2>
<div class="outline-text-2" id="text-orgf733648">
<ul class="org-ul">
<li>Conditional pmf: \(p_y(y | x) = \frac{p_{xy}(x,y)}{p_x(x)}\)</li>
<li>Conditional pdf: \(f_{Y}(y | x) =  \frac{f_{xy}(x,y)}{f_X(x)}\)</li>
<li>Conditional expectation:
\(E[Y | x] = \int_{-\infty}^{\infty}y f_Y(y | x) dy\)
<ul class="org-ul">
<li>This is a function of \(x\)</li>
<li>\(E[Y | X]\) is a random variable
<ul class="org-ul">
<li>perform random experiment, get a value \(X = x_0\), which gives \(E[Y | x_0]\).
Note \(E[Y] = E[E[Y|X]]\)</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgea642fa" class="outline-2">
<h2 id="orgea642fa">More linear transformations:</h2>
<div class="outline-text-2" id="text-orgea642fa">
<p>
If \(Z = A X\) then \(f_z(z) = f_x(A^{-1}z)/det(A)\)
</p>

<p>
Jointly Gaussian Random Variables:
</p>
<ul class="org-ul">
<li>Jointly gaussian random variables have a pdf of a specific form:</li>
</ul>
\begin{equation}
f_x(x) = \frac{e^{-\frac{1}{2}(x-m)^TK^{-1}(x-m)}}{(2\pi)^{n/2}|K|^{\frac{1}{2}}}
\end{equation}

<ul class="org-ul">
<li>pdf is completely specified by mean and covariance</li>
<li>Margin distributions are guassian</li>
<li>Conditional pdf are gaussian</li>
</ul>
</div>
</div>

<div id="outline-container-orgd231de5" class="outline-2">
<h2 id="orgd231de5">Conditional Independence</h2>
<div class="outline-text-2" id="text-orgd231de5">
<p>
\(P(A \cap B | C) = P(A | C)P(B |C)\)
Also:
\(P(A | B, C) = P(A |C)\) and \(P(B | A, C) = P(B |C)\).
</p>

<p>
Not equivalent to independence.
</p>
</div>
</div>

<div id="outline-container-org8e7551f" class="outline-2">
<h2 id="org8e7551f">Resources</h2>
<div class="outline-text-2" id="text-org8e7551f">
<ol class="org-ol">
<li><i>Probability, Statistics, and Random Processes for Electrical Enginering</i>, Alberto Leon-Garcia</li>
<li><a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayes Theorem by 3Blue1Brown</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<p><p class="outline-2">Author: Matthew Elwin</p></p>
</div>
</body>
</html>
