<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-02-24 Sat 20:41 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Kalman Filters</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Matthew Elwin">
<link rel="stylesheet" href="./../pubme.css" type="text/css"/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="./../index.html"> HOME </a>
</div><div id="content">
<header>
<h1 class="title">Kalman Filters</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgb0e62b9">Setup</a></li>
<li><a href="#org6922296">The Results</a>
<ul>
<li><a href="#org663bed7">Initialize</a></li>
<li><a href="#org0ceb036">Predict</a></li>
<li><a href="#org096d860">Correct</a></li>
<li><a href="#orga8172a2">Repeat</a></li>
</ul>
</li>
<li><a href="#org58ec696">Extended Kalman Filter</a>
<ul>
<li><a href="#org3fbfd20">EKF Filtering Equations</a></li>
</ul>
</li>
<li><a href="#org2d29956">Further Reading</a></li>
</ul>
</div>
</nav>
<div id="outline-container-orgb0e62b9" class="outline-2">
<h2 id="orgb0e62b9">Setup</h2>
<div class="outline-text-2" id="text-orgb0e62b9">
<ol class="org-ol">
<li>Linear state space system:</li>
</ol>

\begin{align}
x_k &= Ax_{k-1} + Bu_{k-1} + w_{k-1} \tag{1} \\
z_k &= Hx_k + v_k \tag{2}
\end{align}

<ol class="org-ol">
<li>The state at time \(k\) is \(x_k\) and the measurement at that time is \(z_k\)</li>
<li>The inputs are known.</li>
<li>Initial state \(x_0 \sim \mathcal{N}(q_0, \Sigma_0)\)</li>
<li>Gaussian process noise \(w_k \sim \mathcal{N}(0, Q)\)
<ul class="org-ul">
<li>Noise at each step is independent (independent, identicially distributed or i.i.d.).</li>
</ul></li>
<li>Gaussian sensor noise  \(v_k \sim \mathcal{N}(0, R)\)
<ul class="org-ul">
<li>(i.i.d. noise)</li>
</ul></li>
<li>Noise is independent of state.
<ul class="org-ul">
<li>Can also be just a known correlation, but we simplify here</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org6922296" class="outline-2">
<h2 id="org6922296">The Results</h2>
<div class="outline-text-2" id="text-org6922296">
</div>
<div id="outline-container-org663bed7" class="outline-3">
<h3 id="org663bed7">Initialize</h3>
<div class="outline-text-3" id="text-org663bed7">
<p>
An initial Gaussian distribution representing your knowledge of the initial state.
The Gaussian is \(\mathcal{N}(\hat{x}_0, \Sigma_0)\).
</p>
</div>
</div>

<div id="outline-container-org0ceb036" class="outline-3">
<h3 id="org0ceb036">Predict</h3>
<div class="outline-text-3" id="text-org0ceb036">
<p>
Propagate the state and uncertainty forward:
</p>
\begin{align}
\hat{x}_k^- &= A\hat{x}_{k-1} + Bu_{k-1} \\
\Sigma_k^- &= A\Sigma_{k-1}A^T + Q
\end{align}

<ul class="org-ul">
<li>The prediction random variable is Gaussian: \(X_k^- \sim \mathcal{N}(A\hat{x}_{k-1} + Bu_{k-1}, A\Sigma_{k-1}A^T + Q)\)</li>
</ul>
</div>

<details id="org7deaf26"><summary class="header-4">Derivation</summary>
<div class="outline-text-4" id="text-org7deaf26">
<ol class="org-ol">
<li>For \(X \sim \mathcal{N}(\mu, \Sigma)\), and \(Y = AX + b\), \(Y \sim \mathcal{N}(A\mu +b, A \Sigma A^T)\) (See <a href="./gaussian.html">Lecture on Gaussians</a>)</li>
<li><p>
In this case, the state update equation can be written as:
</p>
\begin{align}
 x_k 
 =
 \begin{bmatrix}
  A & I 
 \end{bmatrix}
 \begin{bmatrix}
 x_{k-1} \\
 w_{k-1}
 \end{bmatrix}
 +
  B u_{k-1}
\end{align}</li>
<li><p>
Applying the formula for a linear transformation of Gaussian random variables reveals the mean to be:
</p>
\begin{align}
\hat{x}_k^- &= \begin{bmatrix} A & I \end{bmatrix} \begin{bmatrix}\hat{x}_{k-1} \\ 0 \end{bmatrix} + Bu_{k-1} \\
       &= A \hat{x}_{k-1} + Bu_{k-1}
\end{align}</li>
<li><p>
Applying the formula for a linear transformation of Gaussian random variables reveals the covariance to be:
</p>
\begin{align}
\Sigma_k^- &= \begin{bmatrix} A & I \end{bmatrix} \begin{bmatrix} \Sigma_{k-1} & 0 \\ 0 & Q \end{bmatrix} \begin{bmatrix} A^T \\ I \end{bmatrix} \\
           &= \begin{bmatrix} A & I \end{bmatrix} \begin{bmatrix} \Sigma_{k-1} A^T \\ Q \end{bmatrix} \\
           &=  A \Sigma_{k-1} A^T + Q
\end{align}
<ul class="org-ul">
<li>Here we used the fact that \(w\) and \(x\) are independent random variables, which is why \(Cov(x_{k-1}, w_{k-1})\) is block diagonal.</li>
</ul></li>
</ol>
</div>
</details>
</div>
<div id="outline-container-org096d860" class="outline-3">
<h3 id="org096d860">Correct</h3>
<div class="outline-text-3" id="text-org096d860">
<p>
Update the estimate based on the sensor measurement.
The Kalman gain \(K\) controls the relative importance of the estimate
from the model and from the measurement
</p>
\begin{align}
K_k &= \Sigma_k^{-}H^T(H \Sigma_k^{-} H^T + R)^{-1}\\
\hat{x}_k &= \hat{x}_k^- + K_k(z(k) - H\hat{x}_k^-)\label{eq:mean_update}\\
\Sigma_k &= (I - K_kH)\Sigma_k^-\label{eq:var_update}
\end{align}

<ul class="org-ul">
<li>The mean and the covariance estimate of the state are provided.</li>
<li>Since the inputs are Gaussian and the system is linear, the estimate
is also a Gaussian distribution.</li>
</ul>
</div>

<details id="orgfccf676"><summary class="header-4">Derivation</summary>
<div class="outline-text-4" id="text-orgfccf676">
<ol class="org-ol">
<li>Let \(Y \sim X_1 | X_2\) = a, where \(\begin{bmatrix}X_1 \\ X_2\end{bmatrix} \sim \mathcal{N}(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix}, \begin{bmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}\end{bmatrix})\) are Jointly Gaussian.
Then
\begin{equation}
Y &sim; \mathcal{N}(&mu;_1 + &Sigma;_{12}&Sigma;_{22}^{-1}(a - &mu;_2),\;&Sigma;_{11}- &Sigma;_{12}&Sigma;_{22}^{-1}&Sigma;_{21}).
\end{equation} (See <a href="./gaussian.html">Lecture on Gaussians</a>)</li>
<li><p>
The measurement and state variable is written in terms of the predicted state and the measurement noise
</p>
\begin{equation}
 \begin{bmatrix}
  x_k \\
  z_k \\
 \end{bmatrix}
  =
  \begin{bmatrix}
     I &  0 \\
     H &  I
     \end{bmatrix}
 \begin{bmatrix}
      x_k \\
      v_k
 \end{bmatrix}
\end{equation}</li>
<li>The Jointly Gaussian random vector  
\(\begin{bmatrix}
         x_k \\
         v_k
    \end{bmatrix} \sim 
    \mathcal{N}(\begin{bmatrix}\hat{x}_k^- \\ 0 \end{bmatrix}, \begin{bmatrix} \Sigma_k^- & 0 \\ 0 & R \end{bmatrix})\)</li>
<li><p>
Next, apply the formula for a linear transformation of a Jointly Gaussian random vector to reveal the joint distribution
</p>
\begin{align}
 \begin{bmatrix}
  x_k \\
  z_k \\
 \end{bmatrix} &\sim \mathcal{N}(
   \begin{bmatrix}
    I & 0 \\
    H & I
    \end{bmatrix}
    \begin{bmatrix}
     \hat{x}_k^- \\
     0
    \end{bmatrix},

   \begin{bmatrix}
    I & 0 \\
    H & I
    \end{bmatrix}
    \begin{bmatrix} \Sigma_k^- & 0 \\ 0 & R \end{bmatrix}
   \begin{bmatrix}
    I & H^T \\
    0 & I
    \end{bmatrix}
      )\\
     &\sim \mathcal{N}(
      \begin{bmatrix}
       \hat{x}_k^- \\
        H\hat{x}_k^-
      \end{bmatrix},
       \begin{bmatrix}
       \Sigma_k^- & \Sigma_k^- H^T \\
       H \Sigma_k^- & H \Sigma_k^- H^T + R
       \end{bmatrix})
\end{align}</li>

<li>Applying the formula for the conditional Jointly Gaussian random vector to \(\begin{bmatrix}x_k\\z_k\end{bmatrix}\), when \(z_k = z(k)\)
reveals that it is distributed with a mean given by equation \eqref{eq:mean_update} and variance given by equation \eqref{eq:var_update}.</li>
</ol>
</div>
</details>
</div>
<div id="outline-container-orga8172a2" class="outline-3">
<h3 id="orga8172a2">Repeat</h3>
<div class="outline-text-3" id="text-orga8172a2">
<ul class="org-ul">
<li>The results from the correction step (\(\hat{x}_{k-1}, \Sigma_{k-1}\)) are used for the prediction at time \(k\)</li>
<li>Thus, Kalman filtering consists of a repeated cycle of prediction and correction steps</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org58ec696" class="outline-2">
<h2 id="org58ec696">Extended Kalman Filter</h2>
<div class="outline-text-2" id="text-org58ec696">
<ol class="org-ol">
<li><p>
Start with non-linear system, whose state \(x_k\) needs to be estimated from measurements \(z_k\)
</p>
\begin{align}
x_k &= g(x_{k-1}, u_{k-1}, w_{k-1}) \\
z_k &= h(x_k, v_k)
\end{align}

<p>
To approximate the state, set the noise to zero and propagate the previous state
forward to the next timestep:
</p>
\begin{equation}
\tilde{x}_k  = g(\hat{x}_{k-1}, u_{k-1}, 0) 
\end{equation}

<p>
To approximate the measurement that could be expected from a given state use
the previous state estimate and set noise to zero:
</p>
\begin{equation}
\tilde{z}_k  = h(\tilde{x}_k, 0)
\end{equation}</li>

<li><p>
Linearize the system about the previous estimate \(\hat{x}_k\) and 0 noise (using Taylor expansion).
</p>
\begin{align}
x(k) &\approx g(\hat{x}_{k-1}, u_{k-1}, 0) + A_k (x_{k-1} - \hat{x}_{k-1}) +  W_k w_{k-1} \label{eq:taylorx}\\
z(k) &\approx h(\hat{x}_k, 0) + H_k (x_{k} - \tilde{x}_{k}) + V_k  v_k \label{eq:taylorz}
\end{align}
<p>
with
</p>
\begin{align}
A_k &= \frac{d g}{dx}\Bigr\rvert_{\hat{x}_{k-1}, u_{k-1}, 0}\\
W_k &= \frac{d g}{dw}\Bigr\rvert_{\hat{x}_{k-1}, u_{k-1}, 0} \\
H_k &= \frac{d h}{dx}\Bigr\rvert_{\tilde{x}_{k}, 0} \\
V_k &= \frac{d h}{dv}\Bigr\rvert_{\tilde{x}_{k}, 0}
\end{align}

<p>
Let the process noise random variable \(\tilde{w} = W_k w_k\) be distributed as a Gaussian with \(\mathcal{N}(0, W_kQW_k^T)\)
and the sensor noise random variable \(\tilde{v} = V_k v_k \sim \mathcal{N}(0, V_kRV_k^T)\)
</p></li>

<li><p>
The dynamics of the estimation error can be modeled as a linear
system. Thus, a Kalman filter can be used to estimate the estimation error!
</p>

<p>
The error between the state and the prediction is
</p>
\begin{equation}
\tilde{e}_{x_k} = x_k - \tilde{x}_k
\end{equation}

<p>
And the error between the actual measurement and estimated measurement (the residual) is
</p>
\begin{equation}
\tilde{e}_{z_k} = z_k - \tilde{z}_k
\end{equation}

<p>
Substituting the linear approximations in equations \eqref{eq:taylorx} and \eqref{eq:taylorz} into the error equations yields the first-order approximate error dynamics
</p>
\begin{align}
\tilde{e}_{x_k} &\approx A_k(x_{k-1} - \hat{x}_{k-1}) + \tilde{w}_{k-1} \\
\tilde{e}_{z_k} & \approx H_k(x_{k} - \tilde{x}_{k}) + \tilde{v}_k
\end{align}
<p>
(note that \(\tilde{x}_k = g(\hat{x}_{k-1}, u_k, 0)\) and \(\tilde{z}_k = h(\hat{x}_k, 0)\)).
</p>

<p>
If we use \(\tilde{x}_{k-1}\) as the estimate \(\hat{x}_{k-1}\), the equations above become state equations in \(\tilde{e}_{x_k}\)
</p>
\begin{align}
\tilde{e}_{x_k} &\approx A_k\tilde{e}_{x_{k-1}} + \tilde{w}_{k-1} \\
\tilde{e}_{z_k} & \approx H_k\tilde{e}_{x_k} + \tilde{v}_k,
\end{align}
<p>
Then \(\tilde{e}_{x_k}\) can be estimated by a Kalman filter, yielding an estimate \(\hat{e}_k\).  
The linear system has system matrices due to the linearization and the noise covariance has also been transformed.
</p></li>

<li><p>
Estimating the error is estimating the state:
The Kalman filter estimate of the error is
</p>
\begin{equation}
\hat{e}_k = \hat{e}_k^- + \tilde{K}\tilde{e}_{z_k}.
\end{equation}

<p>
Let \(\hat{e}_k^-\) equal zero (because we predict that the mean of the error is zero) 
</p>
\begin{equation}
\hat{e}_k = K\tilde{e}_{z_k}
\end{equation}

<p>
From the error equation, we substitute \(\hat{x}_k\) for \(x\) and \(\hat{e}_k\) for \(\tilde{e}_{x_k}\)
</p>
\begin{align}
\hat{x}_k &= \hat{e}_k + \tilde{x}_k \\
       &= \tilde{x}_k + K\tilde{e}_{z_k} \\
       &= \tilde{x}_k + K(z_k - \tilde{z}_k)
\end{align}

<p>
Therefore, finding the mean of the error for the linear error system is the same
as finding the mean of the state for that same system shifted by the prediction \(\tilde{x}_k\).
What about the covariance? (exercise)
</p>

<p>
Thus, we use the linearized system for computing the Kalman gain and the measurement update.
However, the prediction \(\tilde{x}_k\) is approximated with \(g(\hat{x}_{k-1},u_{k-1},0)\) and
the predicted measurement \(\tilde{z}_k\) is approximated with \(h(\tilde{x}_k, 0)\).
Uncertainty is propagated using the linearized model.
</p></li>
</ol>
</div>

<div id="outline-container-org3fbfd20" class="outline-3">
<h3 id="org3fbfd20">EKF Filtering Equations</h3>
<div class="outline-text-3" id="text-org3fbfd20">
<ul class="org-ul">
<li>I am using \(A_k\), \(H_k\), \(W_k\) and \(V_k\) as the linearized matrices.</li>
</ul>

<p>
Prediction:
</p>
\begin{align}
\tilde{x}_k &= g(\hat{x}_{k-1}, u_{k-1}, 0)\\
\Sigma_k^- &= A_k\Sigma_{k-1}A_k^T+W_k Q_{k-1} W_k^T
\end{align}

<p>
Correction
</p>
\begin{align}
K &=\Sigma_k^-H_k^T(H_k\Sigma_k^-H_k^T + V_k R_k V_k^T)^{-1}\\
\hat{x}_k &= \tilde{x}_k + K_k(z(k) - h(\tilde{x}_k,0))\\
\Sigma_k &= (I - K H_k)\Sigma_k^-
\end{align}
</div>
</div>
</div>



<div id="outline-container-org2d29956" class="outline-2">
<h2 id="org2d29956">Further Reading</h2>
<div class="outline-text-2" id="text-org2d29956">
<p>
<a href="https://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf">An Introduction to the Kalman Filter (Welch, Bishop 2005, UNC_Chapel Hill TR 95-041)</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p><p class="outline-2">Author: Matthew Elwin</p></p>
</div>
</body>
</html>
