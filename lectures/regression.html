<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-02-06 Tue 13:19 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Regression</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Matthew Elwin">
<link rel="stylesheet" href="./../pubme.css" type="text/css"/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="./../index.html"> HOME </a>
</div><div id="content">
<header>
<h1 class="title">Regression</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org51d74d4">Supervised Learning</a>
<ul>
<li><a href="#org2cd6878">Applications</a></li>
</ul>
</li>
<li><a href="#org91be7d2">Linear Regression</a>
<ul>
<li><a href="#org7e360e1">The Model</a></li>
<li><a href="#org9222700">Matrix Form</a></li>
<li><a href="#org3edce35">Least Squares</a>
<ul>
<li><a href="#org120f746">Minimizing with the SVD</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org89b7296">Linear Regression, Gaussian Noise</a>
<ul>
<li><a href="#org16aa742">The Model</a></li>
<li><a href="#org54c37f0">Finding the Weights</a></li>
</ul>
</li>
<li><a href="#org59f9002">Non-linear functions:</a>
<ul>
<li><a href="#org297ed50">Some Manipulations</a></li>
</ul>
</li>
<li><a href="#orgc9c3dbc">Summary</a></li>
<li><a href="#org356885d">Random Process</a>
<ul>
<li><a href="#orgc1713aa">Another look at linear regression</a></li>
</ul>
</li>
<li><a href="#org647ed93">Model Selection and Hyperparameters</a></li>
<li><a href="#orgfa82c15">Resources</a></li>
</ul>
</div>
</nav>

<div id="outline-container-org51d74d4" class="outline-2">
<h2 id="org51d74d4">Supervised Learning</h2>
<div class="outline-text-2" id="text-org51d74d4">
<p>
Training set of \(n\) observations: \(\mathcal{D} = \{(x_1, y_1) \dots (x_n, y_n)\}\).
</p>

<p>
The input vectors are \(x_i \in \mathbb{R}^D\) and the output (dependent variable) is \(y_i \in \mathbb{R}\).
</p>


<p>
Let \(X \in \mathbb{R}^{D \times n}\) be the design matrix: column \(i\) of \(X\) corresponds to the input vector \(x_i\):
</p>
\begin{equation}
 X = \begin{bmatrix} x_1 & \dots & x_n \end{bmatrix} \in \mathbb{R}^{D \times n}
\end{equation}
<p>
and let \(y \in \mathbb{R}^n\) be a vector where each element is the output:
</p>
\begin{equation}
y = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}.
\end{equation}

<p>
Given a previously unseen input \(x_\ast\) can we predict the output \(y_\ast\)?
</p>
<ul class="org-ul">
<li>Given \(\mathcal{D}\) find \(y = f(x)\)</li>
</ul>
</div>

<div id="outline-container-org2cd6878" class="outline-3">
<h3 id="org2cd6878">Applications</h3>
<div class="outline-text-3" id="text-org2cd6878">
<ul class="org-ul">
<li>Depth data from monocular images</li>
<li>Mapping terrain</li>
<li>Mapping an environmental quantity (such as temperature)</li>
<li>Classification</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org91be7d2" class="outline-2">
<h2 id="org91be7d2">Linear Regression</h2>
<div class="outline-text-2" id="text-org91be7d2">
</div>
<div id="outline-container-org7e360e1" class="outline-3">
<h3 id="org7e360e1">The Model</h3>
<div class="outline-text-3" id="text-org7e360e1">
<ol class="org-ol">
<li>\(y = w_0 + w_1 x_1 + \cdots + w_n x_n\).  The goal is to find the weights \(w\).</li>
<li>By setting \(x_0 = 1\), we can write the model as \(y = w^T x\).</li>
</ol>
</div>
</div>

<div id="outline-container-org9222700" class="outline-3">
<h3 id="org9222700">Matrix Form</h3>
<div class="outline-text-3" id="text-org9222700">
<ol class="org-ol">
<li>Stack all the \(x\) vectors: \(X = \begin{bmatrix}x_1 & \dots & x_n\end{bmatrix}\)</li>
<li>Stack all the measurements \(y = \begin{bmatrix}y_1 & \dots y_n \end{bmatrix}\)</li>
<li><p>
The overall equation is
</p>
\begin{equation}
y = X^T w
\end{equation}</li>
<li>Note that in general, \(X^T \in \mathbb{R}^{n \times D}\)</li>
</ol>
</div>
</div>

<div id="outline-container-org3edce35" class="outline-3">
<h3 id="org3edce35">Least Squares</h3>
<div class="outline-text-3" id="text-org3edce35">
<p>
The regression equation \(y = X^T w\) cannot be solved precisely because there are more measurements than independent variables.
Instead we try to minimize the square error:
</p>

\begin{equation}
w_\ast = \mathrm{arg}\min_w ||X^Tw - y||_2^2
\end{equation}

<p>
So \(w_\ast = (X X^T)^{-1} X y\) when \(n > D\) (there are more measurements then the dimension of the data \(x\)).
</p>
</div>

<div id="outline-container-org120f746" class="outline-4">
<h4 id="org120f746">Minimizing with the SVD</h4>
<div class="outline-text-4" id="text-org120f746">
<ol class="org-ol">
<li>Take the Singular Value Decomposition (SVD) of \(X^T \in \mathbb{R}^{n \times D}\): 
<ol class="org-ol">
<li>\(X^T = U \Sigma V^T\).</li>
<li>All matrices have an SVD.</li>
<li>\(U \in \mathbb{R}^{n \times n}\), \(U^T U = U U^T = I\) (\(U\) is an orthogonal matrix).</li>
<li>\(V \in \mathbb{R}^{D \times D}\), \(V^T V = V V^T = I\) (\(V\) is an orthogonal matrix).</li>
<li>\(\Sigma = \begin{bmatrix} \bar{\Sigma} & 0 \\ 0 & 0\end{bmatrix} \in \mathbb{R}^{n \times D}\), where \(r = \mathrm{rank{X}}\)
\(\bar{\Sigma} \in \mathbb{R}^{r \times r}\) is diagonal with positive diagonal entries.</li>
</ol></li>

<li>We have \(||U A ||_2^2 = ||A||_2^2\) for orthogonal matrix \(U\) because
<ul class="org-ul">
<li>\(||U A||_2 = (UA)^T(UA) = A^T U^T UA = A^TA = ||A||_2^2\)</li>
</ul></li>
<li>Also, \(||X^Tw - y||_2^2 = ||U \Sigma V^T w -y ||_2^2 = || U^T (U \Sigma V^T w -y) ||_2^2 = ||\Sigma V^T w -U^Ty||_2^2\)</li>
<li><p>
In block form the equations are separated
</p>
\begin{align}   
||\begin{bmatrix} \bar{\Sigma} & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix}V_R^T \\ V_N^T\end{bmatrix} w - \begin{bmatrix}U_R^T \\ U_N^T\end{bmatrix}y||_2^2 &= \\
&= || \begin{bmatrix}\bar{\Sigma}V_R^T w - U_R^T y \\ - U_N^T y \end{bmatrix}||_2^2 \\
&=\begin{bmatrix}(\bar{\Sigma}V_R^T w - U_R^T y)^T & (- U_N^T y)^T \end{bmatrix}\begin{bmatrix}\bar{\Sigma}V_R^T w - U_R^T y \\ - U_N^T y \end{bmatrix}\\
&=||\bar{\Sigma}V_R^T w - U_R^T y||_2^2 + ||U_N^Ty||_2^2
\end{align}</li>

<li>The above is minimized when \(w = V_R\bar{\Sigma}^{-1}U_R^Ty\)</li>
<li>When \(n > D\), then \(\mathrm{rank}(X^T) = D\) and thus \(V = V_R\) and \(X^T = \begin{bmatrix}U_R & U_N\end{bmatrix}\begin{bmatrix}\bar{\Sigma} \\ 0\end{bmatrix}V_R^T = U_R \bar{\Sigma}V_R\)
<ul class="org-ul">
<li>Therefore, \((X X^T)^{-1}X = (V_R \bar{\Sigma} U_R^T U_R \bar{\Sigma} V_R^T)^{-1} V_R \bar{\Sigma} U_R^T = V_R \bar{\Sigma}^{-1}\bar{\Sigma}^{-1}V_R^{T}V_R \bar{\Sigma}U_R^{T}= V_R\bar{\Sigma}^{-1}U_R^{T}\)</li>
</ul></li>
</ol>
</div>
</div>
</div>
</div>


<div id="outline-container-org89b7296" class="outline-2">
<h2 id="org89b7296">Linear Regression, Gaussian Noise</h2>
<div class="outline-text-2" id="text-org89b7296">
</div>
<div id="outline-container-org16aa742" class="outline-3">
<h3 id="org16aa742">The Model</h3>
<div class="outline-text-3" id="text-org16aa742">
<ol class="org-ol">
<li>The model: \(y_i = f(x_i) + \epsilon_i\), where \(f(x) = x^T w\) and \(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\).
<ul class="org-ul">
<li><p>
Noise is independent of the function value and other noise values: that is
</p>
\begin{equation}
\epsilon = \begin{bmatrix}\epsilon_1 \\ \vdots \\ \epsilon_n\end{bmatrix} \sim \mathcal{N}(0, \sigma^2 I_{n \times n})
\end{equation}</li>
<li>The goal of the regression is to find \(f(x)\), which in this case consists of finding the weights \(w \in \mathbb{R}^D\)</li>
<li><p>
Without seeing any data, the weights are randomly distributed: 
</p>
\begin{equation}
 w \sim \mathcal{N}(0, \Sigma)
 \end{equation}</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org54c37f0" class="outline-3">
<h3 id="org54c37f0">Finding the Weights</h3>
<div class="outline-text-3" id="text-org54c37f0">
<ol class="org-ol">
<li><p>
Write the posterior distribution of the weights using Bayes rule: 
</p>
\begin{equation}
  p(w | y, X) = \frac{p(y | X, w) p(w)}{p(y | X)}
  \end{equation}
<ul class="org-ul">
<li>We can apply Bayes rule directly, because we know \(p(w)\) and can compute \(p(y | X, w)\) from the formula for the model.</li>
</ul></li>
<li>The likelihood is \(p(y | X, w) = \mathcal{N}(X^T w, \sigma^2 I_{n \times n})\).
<ul class="org-ul">
<li><p>
This formula follows from the independence of each \(\epsilon_i\) and because \(y_i\) is a linear transform of a Gaussian random variable (\(\epsilon\)):
</p>
\begin{align}
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix}
&= \begin{bmatrix}
x_1^T \\
\vdots \\
x_n^T
\end{bmatrix}
w
+ 
\begin{bmatrix}
\epsilon_1 \\
\vdots \\
\epsilon_n
\end{bmatrix}\\
y &= X^T w + \epsilon
\end{align}</li>
</ul></li>
<li><p>
The normalization constant (marginal likelihood) can be computed as
</p>
\begin{align}
p(y | X) &= \int_{-\infty}^{\infty}p(y, w | x)dw  \\
& = \int_{-\infty}^{\infty}p(y | x, w)p(w)dw
\end{align}</li>
<li><p>
After substituting the distributions into Bayes rule, we find
</p>
\begin{equation}
p(w | y, X) \sim \mathcal{N}(\bar{w}, A^{-1}),
\end{equation}
<p>
with
</p>
\begin{equation}
A = \frac{1}{\sigma^2}X X^T + \Sigma^{-1}
\end{equation}
<p>
and
</p>
\begin{equation}
\bar{w}= \frac{1}{\sigma^2}A^{-1} X y.
\end{equation}</li>
<li><p>
To make predictions from the model&#x2014;that is, what is \(y_\ast = f(x_\ast)\) given \(x_\ast\)&#x2014;
use the probability distribution
</p>
\begin{align}
p(y_\ast | x_\ast, X, y) &=
 \int_{-\infty}^{\infty}p(y_\ast, w | x_\ast, X, y) dw \\
 &=\int_{-\infty}^{\infty}p(y_\ast | x_\ast, w)p(w | X, y) dw\\
  &= \mathcal{N}(\frac{1}{\sigma^2}x_\ast^T A^{-1} X y, x_\ast^T A^{-1}x_\ast)\\
  &= \mathcal{N}(x_\ast^T \bar{w}, x_\ast^T A^{-1}x_\ast)
\end{align}

<ul class="org-ul">
<li>The mean of the prediction is just the weighted sum of elements in the input vector,
where the weights are the most probable weights</li>
<li>Variance grows with magnitude of input</li>
</ul></li>
</ol>
</div>
</div>
</div>


<div id="outline-container-org59f9002" class="outline-2">
<h2 id="org59f9002">Non-linear functions:</h2>
<div class="outline-text-2" id="text-org59f9002">
<ul class="org-ul">
<li>Instead of using \(x_i\), for each input data point, map to an \(N\) dimensional feature
space \(\phi(x_i)\). Since \(\phi(x_i)\) does not depend on weights, we can just replace
\(x_i\) with \(\phi(x_i)\) in the previous expression (and let \(\Phi(X)\) be \(\phi\) applied to the columns of \(x\)).</li>
<li>Thus, for non-linear basis functions, we get
\begin{equation}
p(f_&lowast; | x_&lowast;, X, y) = \mathcal{N}(\frac{1}{\sigma^2}&phi;(x_&lowast;)^T A^{-1}&Phi; y, &phi;(x_&lowast;)^T A^{-1} &phi;(x_&lowast;)).
\end{equation}.</li>
<li>Here \(A = \frac{1}{\sigma^2}\Phi \Phi^T + \Sigma^{-1}\).</li>
</ul>
</div>

<div id="outline-container-org297ed50" class="outline-3">
<h3 id="org297ed50">Some Manipulations</h3>
<div class="outline-text-3" id="text-org297ed50">
<ul class="org-ul">
<li>In its original form, \(p(f_\ast | x_\ast, X, y)\) requires inverting an \(N \times N\) matrix
<ul class="org-ul">
<li>With some manipulation we can make it so that we only need to invert an \(n \times n\) matrix.</li>
</ul></li>
<li>\(A \in \mathbb{R}^{N \times N}\), that is its dimensions scale with the number of features</li>
<li><p>
Let \(K = \Phi^T \Sigma \Phi\).  Then note that
</p>
\begin{equation}
A^{-1} \Phi = \Sigma \Phi (K + \sigma^2 I)^{-1}
\end{equation}
<p>
and (from the Matrix Inversion Lemma a.k.a Woodbury Matrix Identity)
</p>
\begin{equation}
\phi_\ast^T A^{-1} \phi_\ast = \phi_\ast^T \Sigma \phi - \phi_\ast \Sigma \Phi(K + \sigma^2 I)^{-1}\Phi^T \Sigma \phi_\ast,
\end{equation}</li>
<li><p>
After applying these transformations, the estimate becomes
</p>
\begin{equation}
\mathcal{N}(\phi_\ast^T\Sigma\Phi(K + \sigma^2I)^{-1}y,
\phi_\ast^T\Sigma\phi_\ast - \phi_\ast^T\Sigma\Phi(K + \sigma^2)^{-1}\Phi^T\Sigma\phi_\ast
\end{equation}</li>

<li>Hand-wave thought about dimensionality: 
<ul class="org-ul">
<li>We have an infinite number of basis functions \(N \to \infty\) with a finite number of inputs \(n\).</li>
</ul></li>
<li>The covariance kernel \(k(x, x') = \phi(x)^T \Sigma \phi(x')\).</li>
<li>Note that \(k(x, x')\) is an inner product since \(\Sigma > 0\).</li>
<li>The kernel trick:
<ul class="org-ul">
<li>A computational trick</li>
<li>If we know \(k(x, x')\), we can replace all these feature vector evaluations directly by using the kernel function on the original inputs.</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc9c3dbc" class="outline-2">
<h2 id="orgc9c3dbc">Summary</h2>
<div class="outline-text-2" id="text-orgc9c3dbc">
<ul class="org-ul">
<li>Given labeled training data</li>
<li>Map inputs into a feature space</li>
<li>Given a prior on the weights and some noise statistics, we can predict
the distribution of the output given a new input</li>
<li>Using the kernel trick avoids needing to explicitly evaluate the features.</li>
</ul>
</div>
</div>

<div id="outline-container-org356885d" class="outline-2">
<h2 id="org356885d">Random Process</h2>
<div class="outline-text-2" id="text-org356885d">
<ol class="org-ol">
<li>A random vector is a vector of random variables with a joint distribution
<ul class="org-ul">
<li>For example, Jointly Gaussian random vector</li>
</ul></li>
<li>(Loosely) when the length of a random vector grows to infinity it becomes a random process</li>
<li>A random process is an infinite collection of random variables (indexed by elements of some set) on the same probability space
<ul class="org-ul">
<li>Any finite collection of these random variables has a joint distribution.</li>
</ul></li>
<li>A Gaussian Random process, the joint distribution of any finite subset of the collection has a jointly Gaussian Distribution
<ul class="org-ul">
<li>can be indexed by elements in \(R^n\) (for example, time, spatial dimensions, etc).</li>
<li>Defined by a mean function \(m(x)\) and covariance function \(k(x, x')\).
<ul class="org-ul">
<li>The mean function provides the mean at the given index and the covariance function computes the covariance between two indices.</li>
<li>Since the joint distribution between any two indices is Gaussian, these functions fully specify the distribution</li>
</ul></li>
</ul></li>
<li>In Gaussian process regression, the function we are attempting to learn is assumed to be described by a Gaussian process
<ul class="org-ul">
<li>This is less restrictive then it seems
<ul class="org-ul">
<li>We saw how the linear regression model could be expressed in terms of the covariance function \(k(x, x')\) via the "kernel trick"</li>
<li>Thus, the inner product and the linear basis functions defines a "kernel"</li>
<li>It turns out that we can choose a kernel and that kernel defines some set of basis functions</li>
<li>Some kernels correspond an infinite number of basis functions, thus the kernel enables us to express functions
that have an infinite-dimensional basis in a finite manner.</li>
</ul></li>
</ul></li>
</ol>
</div>

<div id="outline-container-orgc1713aa" class="outline-3">
<h3 id="orgc1713aa">Another look at linear regression</h3>
<div class="outline-text-3" id="text-orgc1713aa">
<p>
Given the mean and covariance of a Gaussian process (and we'll assume mean 0 here), we can write a distribution for the process,
partitioning into points we have measured and points at which we want a prediction
</p>

\begin{equation}
\begin{bmatrix}
f_\ast
y \\
\end{bmatrix}
\sim
\mathcal{N}(0, \begin{bmatrix}
K(X_\ast, X_\ast) & K(X_\ast, X) \\
K(X, X_\ast) & K(X, X)
\end{bmatrix}),
\end{equation}
<p>
where \(K(X, X_\ast)\) is the \(n \times n_\ast\) covariance matrix formed by \(k(x, x_\ast)\) at all training and test point combinations.
</p>

<p>
The above is just a jointly Gaussian distribution, with the prediction points and input points given.
Thus, we can simply apply the formula for a conditional Gaussian to predict everything at \(f_\ast\).
</p>
\begin{equation}
\mathcal{N}(K(X_\ast, X)(K(X, X) + \sigma^2 I)^{-1}y, K(X_\ast, X_\ast) - K(X_\ast, X)(K(X, X) + \sigma^2 I)^{-1}K(X, X_\ast))
\end{equation}
</div>
</div>
</div>

<div id="outline-container-org647ed93" class="outline-2">
<h2 id="org647ed93">Model Selection and Hyperparameters</h2>
<div class="outline-text-2" id="text-org647ed93">
<ul class="org-ul">
<li>The data can be used to select the covariance model and hyperparameters</li>

<li>There is a hierarchy of parameters
<ol class="org-ol">
<li>The actual regression itself (the weights)</li>
<li>The hyperparameters: These are parameters that form part of the covariance function</li>
<li>The model structure itself.</li>
</ol></li>

<li><p>
Apply Bayes rule each time:
</p>
\begin{equation}
p(w | y, X, \theta, H_i) = \frac{p(y | X, W, H_i)p(w|\theta, H_i)}{p(y | X, \theta, H_i)}
\end{equation}

\begin{equation}
p(\theta | y, X, H_i) = \frac{p(y | X, H_i, \theta)p(\theta |H_i)}{p(y | X, H_i)}
\end{equation}</li>
</ul>

<p>
Prior of models \(p(H_i)\) (often flat to not bias the model).
</p>
\begin{equation}
  p(H_i | y, X) = \frac{p(y | X, H_i)p(H_i)}{p(y | X)}
  \end{equation}
\begin{equation}
p(y | X) = \sum_i p(y | X, H_i) p(H_i)
\end{equation}
</div>
</div>


<div id="outline-container-orgfa82c15" class="outline-2">
<h2 id="orgfa82c15">Resources</h2>
<div class="outline-text-2" id="text-orgfa82c15">
<p>
Much of these notes are derived from <i>Gaussian Processes for Machine Learning</i>, Carl Edward Rasmussen and Chris Williams, The MIT Press, 2006
Avaiable at <a href="http://gaussianprocess.org/">http://gaussianprocess.org/</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p><p class="outline-2">Author: Matthew Elwin</p></p>
</div>
</body>
</html>
