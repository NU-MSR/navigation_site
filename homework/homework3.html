<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-01-04 Thu 22:12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Homework 3</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Matthew Elwin">
<link rel="stylesheet" href="./../pubme.css" type="text/css"/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="./../index.html"> HOME </a>
</div><div id="content">
<header>
<h1 class="title">Homework 3</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgfd5a78d">Task F</a>
<ul>
<li><a href="#orgdab44d3">Task F.6 (Laser Scanner)</a></li>
</ul>
</li>
<li><a href="#org4a2feb1">Task V (Visualization)</a>
<ul>
<li><a href="#org22c2eb5">Task V.1 (Simulation Standalone)</a>
<ul>
<li><a href="#orgfc28752">From Previous Homeworks</a></li>
<li><a href="#orgfd61249">To Add</a></li>
</ul>
</li>
<li><a href="#org6c8481a">Task V.2 (Simulation with Odometry)</a></li>
<li><a href="#org3fc3ac4">Task V.3 (Real robot with Odometry)</a></li>
<li><a href="#org11533fd">Task V.4 (Simulated Robot With SLAM)</a></li>
</ul>
</li>
<li><a href="#org5f4a7e3">Task C (Simulation)</a>
<ul>
<li><a href="#org1d57cea">Task C.9 (Robot Position)</a></li>
<li><a href="#org51a7dc7">Task C.10 (Basic Sensor)</a></li>
<li><a href="#org4ba8f02">Task C.11 (Collision Detection)</a></li>
<li><a href="#orga26b5be">Task C.12 (Lidar)</a>
<ul>
<li><a href="#orgf2424dd">Hint</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge4a29f4">Task L (SLAM)</a>
<ul>
<li><a href="#org88708dd">Task L.1 (Main SLAM Results)</a></li>
<li><a href="#org48b3f7a">Task L.2 (SLAM in controlled environment)</a></li>
<li><a href="#org82c5e67">Suggested Structure</a></li>
<li><a href="#orge3ec156">Matrix Library</a></li>
<li><a href="#org62ce2ca">SLAM</a></li>
</ul>
</li>
</ul>
</div>
</nav>
<p>
Remember to follow the <a href="./guidelines.html">Homework Guidelines</a>
</p>

<div id="outline-container-orgfd5a78d" class="outline-2">
<h2 id="orgfd5a78d">Task F</h2>
<div class="outline-text-2" id="text-orgfd5a78d">
<p>
Lasers!
</p>
</div>
<div id="outline-container-orgdab44d3" class="outline-3">
<h3 id="orgdab44d3">Task F.6 (Laser Scanner)</h3>
<div class="outline-text-3" id="text-orgdab44d3">
<ul class="org-ul">
<li>In <code>nuturtle_control</code> <code>start_robot.launch</code> start the <code>hlds_laser_publisher</code> node from the <code>hls_lfcd_lds_driver</code>,
whenever <code>robot</code> is set to <code>localhost</code></li>
<li>The <a href="https://index.ros.org/p/hls_lfcd_lds_driver/github-ROBOTIS-GIT-hls_lfcd_lds_driver/#humble">hls_lfcd_lds_driver</a> package has a launchfile that can be used. The port is <code>/dev/ttyUSB0</code></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org4a2feb1" class="outline-2">
<h2 id="org4a2feb1">Task V (Visualization)</h2>
<div class="outline-text-2" id="text-org4a2feb1">
<ul class="org-ul">
<li>This task gathers all the visualization requirements for each important mode of running the code in this project.</li>
<li>Implementing this task depends on having multiple <code>rviz</code> configuration files, one for each situation.</li>
<li>The general rule of thumb is that if a launchfile starts <code>rviz</code>, that is optional. This way it can be included
in a higher-level launchfile that loads it's own <code>rviz</code> setup.</li>
<li>Some elements have already been implemented in previous assignments, and others will be added in later tasks.</li>
<li>You may find that publishing additional messages/tfs and modifying existing nodes will help accomplish these tasks.</li>
</ul>
</div>

<div id="outline-container-org22c2eb5" class="outline-3">
<h3 id="org22c2eb5">Task V.1 (Simulation Standalone)</h3>
<div class="outline-text-3" id="text-org22c2eb5">
<p>
When the <code>nusim</code> is run in a standalone manner (e.g. <code>ros2 launch nusim nusim</code>, the following should be displayed:
</p>
</div>
<div id="outline-container-orgfc28752" class="outline-4">
<h4 id="orgfc28752">From Previous Homeworks</h4>
<div class="outline-text-4" id="text-orgfc28752">
<ol class="org-ol">
<li>The walls (in red)</li>
<li>The obstacles (in red)</li>
<li>The robot model, at the robot's location (in red)</li>
<li>The color red indicates that the item is the "ground truth", which can only be known from the simulation</li>
</ol>
</div>
</div>

<div id="outline-container-orgfd61249" class="outline-4">
<h4 id="orgfd61249">To Add</h4>
<div class="outline-text-4" id="text-orgfd61249">
<ol class="org-ol">
<li>A <code>nav_msgs/Path</code> (in red) tracing the path of the robot</li>
<li>A MarkerArray (in yellow) displaying the relative \((x,y)\) measurements of the markers
<ul class="org-ul">
<li>These markers are added in task C.11, and will not necessarily align with the red markers due to noise.</li>
<li>If the measured marker cannot be seen by the robot, it should not be displayed</li>
</ul></li>
<li>A <code>sensor_messages/LaserScan</code> displaying the simulated laser scan data (in red)</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org6c8481a" class="outline-3">
<h3 id="org6c8481a">Task V.2 (Simulation with Odometry)</h3>
<div class="outline-text-3" id="text-org6c8481a">
<p>
When the <code>nusim</code> is run with the odometry node (e.g., <code>ros2 launch nuturtle_control start_robot.launch robot:=nusim</code>
</p>

<ol class="org-ol">
<li>Everything from task V.1</li>
<li>The robot model, at the location of the odometry estimate (in blue)</li>
<li>A <code>nav_msgs/Path</code>  corresponding to the path the robot takes according to odometry measurements (in blue)</li>
<li>Blue indicates information learned via odometry</li>
</ol>
</div>
</div>
<div id="outline-container-org3fc3ac4" class="outline-3">
<h3 id="org3fc3ac4">Task V.3 (Real robot with Odometry)</h3>
<div class="outline-text-3" id="text-org3fc3ac4">
<p>
When <code>ros2 launch nuturtle_control start_robot.launch robot:=none</code> (on your computer) and
     <code>ros2 launch nuturtle_control start_robot.launch robot:=localhost</code> (on the turtlebot).
</p>
<ol class="org-ol">
<li>Draw the actual obstacle locations and walls (in red)
<ul class="org-ul">
<li>Add a parameter called <code>draw_only</code> to <code>nusim</code> that causes it to just draw the obstacles from the configuration file rather than simulating anything</li>
<li>When the node is launched under these circumstances, set its name to <code>nuwall</code> to distinguish it from a running simulation</li>
</ul></li>
<li>Draw the robot according to the odometry (in blue)</li>
<li>A <code>nav_msgs/Path</code> corresponding to the path the robot takes according to odometry measurements (in blue)</li>
<li>A sensor_messages/LaserScan displaying the real laser scan data (in Yellow)</li>
</ol>
</div>
</div>

<div id="outline-container-org11533fd" class="outline-3">
<h3 id="org11533fd">Task V.4 (Simulated Robot With SLAM)</h3>
<div class="outline-text-3" id="text-org11533fd">
<p>
When <code>ros2 launch nuslam slam.launch.xml robot:=nusim</code> is used
</p>
<ol class="org-ol">
<li>Everything from Task V.2
<ul class="org-ul">
<li>The <code>odometry</code> node in this task will be broadcasting from <code>odom</code> to <code>green/base_footprint</code>, and there will also be a transform from <code>map-&gt;odom</code></li>
<li>The <code>blue</code> robot should be located where the robot thinks it is according to it's uncorrected odometry in the world frame</li>
<li>Hint: You will need to have an additional <code>tf</code> broadcast from <code>world</code> to <code>blue/base_footprint</code>. You can either modify the <code>odometry</code> node or run two of them to
see where the robot is according to it's odometry, without correction from SLAM. This extra <code>tf</code> is needed because the <code>odometry</code> used by SLAM is
continuously corrected using the sensor measurements</li>
</ul></li>
<li>The map, consisting of the estimated obstacle locations (in green)</li>
<li>The robot (in green) at the estimated robot location (from SLAM)</li>
<li>A <code>nav_msgs/Path</code> corresponding to the path the robot takes according to slam (in green).</li>
<li>Green represents the best estimate of the robot location, according to SLAM</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org5f4a7e3" class="outline-2">
<h2 id="org5f4a7e3">Task C (Simulation)</h2>
<div class="outline-text-2" id="text-org5f4a7e3">
</div>
<div id="outline-container-org1d57cea" class="outline-3">
<h3 id="org1d57cea">Task C.9 (Robot Position)</h3>
<div class="outline-text-3" id="text-org1d57cea">
<p>
We will now add error/noise into the <code>nusim</code> robot simulation.
</p>
<ol class="org-ol">
<li>Create a parameters for the <code>nusim</code> node called <code>input_noise</code> and <code>slip_fraction</code></li>
<li>The following steps should occur whenever the simulator receives a <code>wheel_cmd (nuturtlebot_msgs/msg/WheelCommands)</code> message
<ol class="org-ol">
<li>Let \(u_i\) be the commanded wheel velocity, in <code>rad/sec</code></li>
<li>Let \(v_i = u_i + w_i\), where \(w_i\) is zero mean Guassian noise with variance <code>input_noise</code>, when \(u_i \neq 0\) and \(u_i\) otherwise.
<ul class="org-ul">
<li>When \(u_i = 0\) we are commanding the wheels to stop, and are confident in the ability of the motors to achieve that</li>
<li>However, when commanding other wheel speeds, we may not move exactly as commanded.</li>
</ul></li>
<li>After computing \(v_i\), move the robot accordingly</li>
<li>Let \(\eta_r\) and \(\eta_l\) be uniform random between <code>-slip_fraction</code> and <code>slip_fraction</code>
<ul class="org-ul">
<li>Update the right wheel position as if it had followed \(v_i*(1+\eta_r)\) and the left wheel's position as if it had followed \(u*(1+\eta_l)\)</li>
<li>In this way, the actual wheel positions don't match where they would be if they had not slipped</li>
</ul></li>
</ol></li>
</ol>
</div>
</div>

<div id="outline-container-org51a7dc7" class="outline-3">
<h3 id="org51a7dc7">Task C.10 (Basic Sensor)</h3>
<div class="outline-text-3" id="text-org51a7dc7">
<p>
We will add a sensor that determines the relative \((x,y)\) locations of the obstacles, and assigns each a unique identifier.
</p>

<ol class="org-ol">
<li>At a frequency of 5 Hz, publish a MarkerArray on the <code>/fake_sensor</code> topic. It contains the measured positions of the cylindrical obstacles relative to the robot.
<ul class="org-ul">
<li>The positions of these markers should have zero mean Gaussian noise added to them, with the amount added being specified as parameter <code>basic_sensor_variance</code>.</li>
<li>If a marker is not within a radius <code>max_range</code> of the robot, the action for it should be set to <code>DELETE</code> to make it non-visible.
<ul class="org-ul">
<li><code>max_range</code> should be a settable parameter for <code>nusim</code></li>
</ul></li>
<li>A node can subscribe to these markers and treat them like sensor data, being sure to ignore measurements with an action of <code>DELETE</code></li>
</ul></li>
<li>Optionally, you can scale the marker so that it represents a level-set of the Gaussian distribution caused by the sensor noise, which may help with debugging</li>
</ol>
</div>
</div>

<div id="outline-container-org4ba8f02" class="outline-3">
<h3 id="org4ba8f02">Task C.11 (Collision Detection)</h3>
<div class="outline-text-3" id="text-org4ba8f02">
<ol class="org-ol">
<li>Implement collision detection between the simulated robot and the obstacles
<ul class="org-ul">
<li>Represent the robot as a circle with a fixed collision radius. There should already be a <code>collision_radius</code> parameter in <code>diff_params.yaml</code> (see Homework 2: A.7)</li>
<li>If after updating the robot's position, the robot intersects with a cylinder obstacle
<ul class="org-ul">
<li>Compute the line between the robot center and the obstacle center</li>
<li>Move the robot's center along this line so that the collision circles are tangent</li>
<li>Spin the wheels as if the robot had moved normally</li>
<li>This simulates what happens when a turtlebot collides with a fixed obstacles: the wheels spin but the robot does not move</li>
</ul></li>
</ul></li>
<li>Check for collision each time you update the position of the robot</li>
<li>You may assume that obstacles are spaced far enough apart that all collisions involve exactly one cylinder and the robot.</li>
<li>With this collision implemented, you will be able to crash into obstacles and generate a big disparity between the robot location and the odometry, which
will be useful during testing.</li>
</ol>
</div>
</div>
<div id="outline-container-orga26b5be" class="outline-3">
<h3 id="orga26b5be">Task C.12 (Lidar)</h3>
<div class="outline-text-3" id="text-orga26b5be">
<p>
The goal of this task is to simulate the turtlebot's lidar.
</p>
<ol class="org-ol">
<li>Your <code>nusim</code> simulation node should publish a <code>sensor_msgs/msg/LaserScan</code> message with simulated lidar data at 5Hz
<ul class="org-ul">
<li>You may omit the intensities from the message.</li>
<li>Use values from the real turtlebot's LaserScan message to fill this in by looking at a live message from the turtlebot</li>
</ul></li>
<li>The laser scanner will have Gaussian noise added to the simulated values.</li>
<li>Values for the following constants should be settable with ROS parameters
<ol class="org-ol">
<li>Minimum/maximum range (by default match the real turtlebot)</li>
<li>Angle increment and number of samples (by default match the real turtlebot)</li>
<li>Resolution (by default match the real turtlebot)</li>
<li>Noise level</li>
</ol></li>
<li>The simulated lidar should also return distances to the border wall in the simulator, if a ray hits that wall</li>
</ol>
</div>

<div id="outline-container-orgf2424dd" class="outline-4">
<h4 id="orgf2424dd">Hint</h4>
<div class="outline-text-4" id="text-orgf2424dd">
<ol class="org-ol">
<li>There are many ways of implementing this sensor, but this article on Circle-Line intersection may help <a href="https://mathworld.wolfram.com/Circle-LineIntersection.html">https://mathworld.wolfram.com/Circle-LineIntersection.html</a></li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-orge4a29f4" class="outline-2">
<h2 id="orge4a29f4">Task L (SLAM)</h2>
<div class="outline-text-2" id="text-orge4a29f4">
<ul class="org-ul">
<li>This task is about implementing Feature-Based Kalman Filter SLAM.</li>
<li>You will do this task in a new package called <code>nuslam</code></li>
<li>Task L.1 - outlines the main results for the task.</li>
<li>Task L.2 - This is a subset of L.1 (implementing SLAM in a controlled environment).  Once you implement this, you have
completed the core algorithm.</li>
<li>I am not mandating specific nodes or parameters or a structure for you to use. The design is up to you.</li>
<li>However, following the pattern we've been using in this course of keeping calculations separate from the mechanics of the nodes
may prove helpful</li>
<li>Be sure to incldue in your README all instructions to run SLAM</li>
</ul>
</div>
<div id="outline-container-org88708dd" class="outline-3">
<h3 id="org88708dd">Task L.1 (Main SLAM Results)</h3>
<div class="outline-text-3" id="text-org88708dd">
<ul class="org-ul">
<li>This task is about the main interface for running and visualizing your algorithms.</li>
<li>You can write whatever nodes you want, as long as you can run a <code>slam.launch</code> file as specified below and, where appropriate
view the necessary visualizations.</li>
<li>You may also need to modify some nodes to let them publish appropriate visualization topics</li>
<li>Write a launchfile, <code>slam.launch robot:=X</code> that runs your SLAM algorithm and all other necessary nodes
<ul class="org-ul">
<li>If <code>robot == nusim</code> the robot is simulated by <code>nusim</code>.</li>
<li>In Homework 4, <code>robot == turtlebotname</code> will run the code on the turtlebot.</li>
<li>This launchfile should also start a method to control the robot, I recommend using the <code>turtlebot3_teleop</code> keyboard.</li>
</ul></li>
<li>Visualization specifications are determined in Task V.</li>
</ul>
</div>
</div>

<div id="outline-container-org48b3f7a" class="outline-3">
<h3 id="org48b3f7a">Task L.2 (SLAM in controlled environment)</h3>
<div class="outline-text-3" id="text-org48b3f7a">
<ul class="org-ul">
<li>In this task, you will test the extended Kalman Filter SLAM algorithm in the simulation environment.</li>
<li>This environment enables you to verify the correctness of the algorithm without worrying about data-association or noise.</li>
<li>You should be able to run the code for this task by specifying <code>robot:=nusim</code> with the <code>slam.launch</code> launchfile</li>
<li>In the next assignment, you will be using machine learning to find landmarks from 2D-Lidar data.
<ul class="org-ul">
<li>The data from the simulator, however, has identifiers directly specifying the landmarks.</li>
</ul></li>
<li>The algorithm should compute the pose of the robots and the pose of the landmarks relative to a map frame.</li>
<li>Your SLAM node should publish a transform from <code>map</code> to <code>odom</code> such that <code>map</code> to <code>base_footprint</code> reflects the pose of the robot in the <code>map</code> frame.
<ul class="org-ul">
<li>See <a href="https://www.ros.org/reps/rep-0105.html">ROS REP 105</a></li>
</ul></li>
<li>Implement extended Kalman Filter Slam, assuming a known data association.  (That is you know which measurement goes with which landmark)</li>
<li>Drive the robot along closed path in <code>nusim</code> with several landmarks and take a screenshot of <code>rviz</code> at the end of your run.
<ul class="org-ul">
<li>The screenshot should clearly show the actual robot position, the estimated position, the map frame, the odom frame, and the paths of the real, odometry, and SLAM robots.</li>
<li>You should have noise enabled and a limited detection radius for this test (the radius should be small enough such that you do not see all the landmarks all the time)</li>
<li>The robot should follow a path such that it sees all the landmarks on its journey</li>
</ul></li>
<li>If you set to the simulator to generate no sensor noise and have a large detection radius, the SLAM path and actual path should line-up almost exactly and 
the landmark positions should be correct, even if the odometry noise is still present.
<ul class="org-ul">
<li>A good way to test your algorithm (in simulation!) is to drive your robot into a landmark (to make the odometry be very wrong) and watch the map estimate
remain good.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org82c5e67" class="outline-3">
<h3 id="org82c5e67">Suggested Structure</h3>
<div class="outline-text-3" id="text-org82c5e67">
<p>
The below ideas are my suggestions for how to accomplish this task; however, you may choose to ignore them.
</p>
</div>
</div>
<div id="outline-container-orge3ec156" class="outline-3">
<h3 id="orge3ec156">Matrix Library</h3>
<div class="outline-text-3" id="text-orge3ec156">
<ol class="org-ol">
<li>I recommend using the armadillo library <a href="http://arma.sourceforge.net/">http://arma.sourceforge.net/</a> for C++</li>
<li>Install <code>libarmadillo-dev</code></li>
<li>In cmake, <code>find_package(Armadillo)</code>, then add <code>${ARMADILLO_INCLUDE_DIRS}</code> to the <code>include_directories()</code> and <code>target_link_libraries(${ARMADILLO_LIBRARIES})</code> to targets that use armadillo</li>
</ol>
</div>
</div>

<div id="outline-container-org62ce2ca" class="outline-3">
<h3 id="org62ce2ca">SLAM</h3>
<div class="outline-text-3" id="text-org62ce2ca">
<ol class="org-ol">
<li>You may choose to use the odometry node as-is, or to copy its source code into the <code>nuslam</code> package and modify it to be a <code>slam</code> and <code>odometry</code> node.
<ul class="org-ul">
<li>Keeping it separate is more versatile/realistic, since then somebody could theoretically use your <code>slam</code> node with different <code>odometry</code>
<ul class="org-ul">
<li>It also makes re-using launchfiles easier.</li>
</ul></li>
<li>Making a combined node arguably could be easier to synchronize <code>odometry</code> data with <code>sensor</code> updates.</li>
</ul></li>
<li>Start with the <code>odometery</code> node from a previous homework.  Copy the source code into the <code>nuslam</code> package and call it <code>slam.cpp</code>
<ul class="org-ul">
<li>In this guide, we will implement all SLAM functionality in a single node.</li>
<li>This node will also implement the odometry functionality</li>
<li>This decision makes it easier to keep everything in sync.
<ul class="org-ul">
<li>You need to be careful about synchronization. Since <code>tf</code> must be a tree we can't broadcast the <code>map</code> to <code>green/base_footprint</code> transform directly:
instead, we must publish a <code>map</code> to <code>odom</code> transform such that the <code>map</code> to <code>base_footprint</code> transform is the desired value.
Having all these quantities in the same node makes this process easier</li>
<li>However, you lose flexibility: odometry and SLAM are now coupled: for this project the trade-off between flexibility and complication seems worth it.</li>
</ul></li>
<li>It is also important that the SLAM filter be updated one with odometry and once with the sensor measurements, even though odometry comes in at a faster rate
<ul class="org-ul">
<li>It is easiest to accumulate the odometry within the 5Hz period, and use it to make a single prediction</li>
</ul></li>
</ul></li>
<li>Prior to implementing the algorithm, make sure the visualizations are working 
<ul class="org-ul">
<li>The inverted measurement model can provide a crude map estimate at each timestep, just to help get the visualization working</li>
</ul></li>
<li>It may be helpful to create several simulated worlds with different geometry and landmarks.
<ul class="org-ul">
<li>For example, test on just one landmark with a 1-D linear kalman filter</li>
<li>The goal here would be to estimate just the <code>x</code> position of the robot</li>
<li>After completing this intermediate step you can then expand to the full problem</li>
</ul></li>
<li>It may be helpful to implement a basic Kalman filter first, then an extended Kalman filter
<ul class="org-ul">
<li>The SLAM method we use is a direct application of an extended Kalman Filter</li>
</ul></li>
</ol>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p><p class="outline-2">Author: Matthew Elwin</p></p>
</div>
</body>
</html>
